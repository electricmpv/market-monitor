"""
RSS æ•°æ®æºæ¨¡å— - ç›‘æ§ Redditã€Product Huntã€Hugging Faceã€Y Combinator
æ— éœ€ API Keyï¼Œå®Œå…¨å…è´¹ä¸”ç¨³å®š
"""

import feedparser
import requests
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import logging
import hashlib
from urllib.parse import quote

logger = logging.getLogger(__name__)


class RSSHunter:
    """RSS æ•°æ®æºç›‘æ§å™¨"""
    
    # RSS æºé…ç½®
    RSS_SOURCES = {
        'reddit_localllama': {
            'url': 'https://www.reddit.com/r/LocalLLaMA/new/.rss',
            'name': 'Reddit - LocalLLaMA',
            'category': 'community'
        },
        'reddit_openai': {
            'url': 'https://www.reddit.com/r/OpenAI/new/.rss',
            'name': 'Reddit - OpenAI',
            'category': 'community'
        },
        'reddit_claude': {
            'url': 'https://www.reddit.com/r/Claude_AI/new/.rss',
            'name': 'Reddit - Claude',
            'category': 'community'
        },
        'reddit_cursor': {
            'url': 'https://www.reddit.com/r/Cursor/new/.rss',
            'name': 'Reddit - Cursor',
            'category': 'community'
        },
        'reddit_ml': {
            'url': 'https://www.reddit.com/r/MachineLearning/new/.rss',
            'name': 'Reddit - Machine Learning',
            'category': 'research'
        },
        'huggingface_papers': {
            'url': 'https://huggingface.co/papers/daily',
            'name': 'Hugging Face - Daily Papers',
            'category': 'research',
            'type': 'html'  # ç‰¹æ®Šå¤„ç†
        },
        'ycombinator': {
            'url': 'https://www.ycombinator.com/rss',
            'name': 'Y Combinator - Launches',
            'category': 'startup'
        },
        'producthunt': {
            'url': 'https://www.producthunt.com/feed.xml',
            'name': 'Product Hunt - Daily',
            'category': 'product'
        },
    }
    
    def __init__(self, keywords_config: Dict = None, timeout: int = 10):
        """
        åˆå§‹åŒ– RSS ç›‘æ§å™¨
        
        Args:
            keywords_config: å…³é”®è¯é…ç½®å­—å…¸
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´
        """
        self.keywords_config = keywords_config or {}
        self.timeout = timeout
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def fetch_rss_feed(self, source_key: str) -> List[Dict]:
        """
        è·å– RSS æºæ•°æ®
        
        Args:
            source_key: RSS æºé”®å
            
        Returns:
            æ–‡ç« åˆ—è¡¨
        """
        if source_key not in self.RSS_SOURCES:
            logger.warning(f"Unknown RSS source: {source_key}")
            return []
        
        source = self.RSS_SOURCES[source_key]
        articles = []
        
        try:
            # ç‰¹æ®Šå¤„ç† Hugging Face
            if source.get('type') == 'html':
                articles = self._fetch_huggingface_papers()
            else:
                # æ ‡å‡† RSS å¤„ç†
                feed = feedparser.parse(source['url'])
                
                if feed.bozo:
                    logger.warning(f"RSS è§£æè­¦å‘Š {source_key}: {feed.bozo_exception}")
                
                for entry in feed.entries[:50]:  # é™åˆ¶å‰50æ¡
                    article = self._parse_rss_entry(entry, source)
                    if article:
                        articles.append(article)
            
            logger.info(f"âœ… è·å– {source['name']}: {len(articles)} æ¡")
            return articles
            
        except Exception as e:
            logger.error(f"âŒ è·å– RSS æºå¤±è´¥ {source_key}: {str(e)}")
            return []
    
    def _parse_rss_entry(self, entry, source: Dict) -> Optional[Dict]:
        """
        è§£æ RSS æ¡ç›®
        
        Args:
            entry: RSS æ¡ç›®
            source: æºé…ç½®
            
        Returns:
            è§£æåçš„æ–‡ç« å­—å…¸
        """
        try:
            title = entry.get('title', '')
            summary = entry.get('summary', '')
            link = entry.get('link', '')
            published = entry.get('published', '')
            
            # æå–å‘å¸ƒæ—¶é—´
            try:
                pub_time = datetime(*entry.published_parsed[:6])
            except:
                pub_time = datetime.now()
            
            # ç”Ÿæˆå†…å®¹æŒ‡çº¹
            content_hash = hashlib.md5(
                f"{title}{summary}".encode()
            ).hexdigest()
            
            return {
                'title': title,
                'summary': summary[:500],  # é™åˆ¶é•¿åº¦
                'link': link,
                'source': source['name'],
                'source_key': source.get('category', 'other'),
                'published_at': pub_time.isoformat(),
                'content_hash': content_hash,
                'platform': 'RSS',
                'type': 'article'
            }
        except Exception as e:
            logger.warning(f"è§£æ RSS æ¡ç›®å¤±è´¥: {str(e)}")
            return None
    
    def _fetch_huggingface_papers(self) -> List[Dict]:
        """
        è·å– Hugging Face Daily Papers
        
        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        articles = []
        try:
            url = 'https://huggingface.co/papers'
            response = self.session.get(url, timeout=self.timeout)
            response.raise_for_status()
            
            # ç®€å•çš„ HTML è§£æ (å¦‚æœéœ€è¦æ›´å¤æ‚çš„è§£æï¼Œå¯ä»¥ä½¿ç”¨ BeautifulSoup)
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # æŸ¥æ‰¾è®ºæ–‡æ¡ç›®
            papers = soup.find_all('article', limit=50)
            
            for paper in papers:
                try:
                    title_elem = paper.find('h3')
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = paper.find('a')['href'] if paper.find('a') else ''
                    
                    # è·å–æ‘˜è¦
                    summary_elem = paper.find('p')
                    summary = summary_elem.get_text(strip=True) if summary_elem else ''
                    
                    content_hash = hashlib.md5(
                        f"{title}{summary}".encode()
                    ).hexdigest()
                    
                    articles.append({
                        'title': title,
                        'summary': summary[:500],
                        'link': f"https://huggingface.co{link}" if link else '',
                        'source': 'Hugging Face - Daily Papers',
                        'source_key': 'research',
                        'published_at': datetime.now().isoformat(),
                        'content_hash': content_hash,
                        'platform': 'RSS',
                        'type': 'paper'
                    })
                except Exception as e:
                    logger.debug(f"è§£æè®ºæ–‡å¤±è´¥: {str(e)}")
                    continue
            
            logger.info(f"âœ… è·å– Hugging Face Papers: {len(articles)} ç¯‡")
            return articles
            
        except Exception as e:
            logger.error(f"âŒ è·å– Hugging Face Papers å¤±è´¥: {str(e)}")
            return []
    
    def fetch_all_sources(self) -> Dict[str, List[Dict]]:
        """
        è·å–æ‰€æœ‰ RSS æºæ•°æ®
        
        Returns:
            æŒ‰æºåˆ†ç±»çš„æ–‡ç« å­—å…¸
        """
        all_articles = {}
        
        for source_key in self.RSS_SOURCES.keys():
            articles = self.fetch_rss_feed(source_key)
            all_articles[source_key] = articles
        
        return all_articles
    
    def filter_by_keywords(self, articles: List[Dict], keywords: List[str]) -> List[Dict]:
        """
        æŒ‰å…³é”®è¯è¿‡æ»¤æ–‡ç« 
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            keywords: å…³é”®è¯åˆ—è¡¨
            
        Returns:
            è¿‡æ»¤åçš„æ–‡ç« åˆ—è¡¨
        """
        filtered = []
        keywords_lower = [k.lower() for k in keywords]
        
        for article in articles:
            title_lower = article['title'].lower()
            summary_lower = article['summary'].lower()
            
            # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•å…³é”®è¯
            for keyword in keywords_lower:
                if keyword in title_lower or keyword in summary_lower:
                    filtered.append(article)
                    break
        
        return filtered
    
    def get_recent_articles(self, hours: int = 24) -> List[Dict]:
        """
        è·å–æœ€è¿‘ N å°æ—¶çš„æ–‡ç« 
        
        Args:
            hours: å°æ—¶æ•°
            
        Returns:
            æœ€è¿‘çš„æ–‡ç« åˆ—è¡¨
        """
        cutoff_time = datetime.now() - timedelta(hours=hours)
        all_articles = self.fetch_all_sources()
        
        recent = []
        for source_articles in all_articles.values():
            for article in source_articles:
                try:
                    pub_time = datetime.fromisoformat(article['published_at'])
                    if pub_time > cutoff_time:
                        recent.append(article)
                except:
                    recent.append(article)  # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼Œé»˜è®¤åŒ…å«
        
        # æŒ‰å‘å¸ƒæ—¶é—´æ’åº
        recent.sort(
            key=lambda x: x.get('published_at', ''),
            reverse=True
        )
        
        return recent
    
    def analyze_trends(self, articles: List[Dict]) -> Dict:
        """
        åˆ†æè¶‹åŠ¿
        
        Args:
            articles: æ–‡ç« åˆ—è¡¨
            
        Returns:
            è¶‹åŠ¿åˆ†æç»“æœ
        """
        trends = {
            'total_articles': len(articles),
            'sources': {},
            'top_keywords': {},
            'categories': {}
        }
        
        # ç»Ÿè®¡æ¥æº
        for article in articles:
            source = article.get('source', 'Unknown')
            trends['sources'][source] = trends['sources'].get(source, 0) + 1
            
            category = article.get('source_key', 'other')
            trends['categories'][category] = trends['categories'].get(category, 0) + 1
        
        return trends


class GoogleTrendsMonitor:
    """Google Trends ç›‘æ§å™¨"""
    
    def __init__(self, timeout: int = 10):
        """åˆå§‹åŒ– Google Trends ç›‘æ§å™¨"""
        self.timeout = timeout
        self.session = requests.Session()
    
    def get_trending_searches(self, region: str = 'US') -> List[Dict]:
        """
        è·å– Google Trends çƒ­æœ
        
        Args:
            region: åœ°åŒºä»£ç  (å¦‚ 'US', 'CN', 'GB')
            
        Returns:
            çƒ­æœåˆ—è¡¨
        """
        try:
            # ä½¿ç”¨ Google Trends API (éœ€è¦ pytrends åº“)
            try:
                from pytrends.request import TrendReq
                
                pytrends = TrendReq(hl='en-US', tz=360)
                
                # è·å–å®æ—¶çƒ­æœ
                trending = pytrends.trending_searches(pn=region)
                
                results = []
                for idx, trend in enumerate(trending.iterrows()):
                    results.append({
                        'rank': idx + 1,
                        'keyword': trend[1][0],
                        'source': 'Google Trends',
                        'region': region,
                        'timestamp': datetime.now().isoformat()
                    })
                
                logger.info(f"âœ… è·å– Google Trends ({region}): {len(results)} æ¡")
                return results
                
            except ImportError:
                logger.warning("pytrends åº“æœªå®‰è£…ï¼Œè·³è¿‡ Google Trends")
                return []
        
        except Exception as e:
            logger.error(f"âŒ è·å– Google Trends å¤±è´¥: {str(e)}")
            return []


def main():
    """æµ‹è¯• RSS ç›‘æ§å™¨"""
    logging.basicConfig(level=logging.INFO)
    
    # åˆ›å»ºç›‘æ§å™¨
    hunter = RSSHunter()
    
    # è·å–æ‰€æœ‰æº
    print("\nğŸ” è·å–æ‰€æœ‰ RSS æº...\n")
    all_articles = hunter.fetch_all_sources()
    
    # ç»Ÿè®¡
    total = sum(len(articles) for articles in all_articles.values())
    print(f"\nğŸ“Š æ€»å…±è·å– {total} æ¡æ–‡ç« \n")
    
    # æ˜¾ç¤ºæ ·æœ¬
    for source_key, articles in all_articles.items():
        if articles:
            print(f"\nğŸ“° {RSSHunter.RSS_SOURCES[source_key]['name']} (å‰3æ¡):")
            for article in articles[:3]:
                print(f"  - {article['title'][:60]}...")
    
    # è·å–æœ€è¿‘24å°æ—¶çš„æ–‡ç« 
    print("\n\nâ° æœ€è¿‘24å°æ—¶çš„æ–‡ç« :")
    recent = hunter.get_recent_articles(hours=24)
    print(f"å…± {len(recent)} æ¡\n")
    
    # åˆ†æè¶‹åŠ¿
    trends = hunter.analyze_trends(recent)
    print(f"ğŸ“ˆ è¶‹åŠ¿åˆ†æ:")
    print(f"  æ¥æºåˆ†å¸ƒ: {trends['sources']}")
    print(f"  åˆ†ç±»åˆ†å¸ƒ: {trends['categories']}")


if __name__ == '__main__':
    main()
