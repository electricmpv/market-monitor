"""
å…³é”®è¯é…ç½®åŠ è½½å™¨ - æ”¯æŒ YAML é…ç½®æ–‡ä»¶çš„åŠ¨æ€åŠ è½½
"""

import yaml
import json
import logging
from pathlib import Path
from typing import Dict, List, Any, Optional
from dataclasses import dataclass

logger = logging.getLogger(__name__)


@dataclass
class KeywordsConfig:
    """å…³é”®è¯é…ç½®æ•°æ®ç±»"""
    pain_radar: Dict[str, List[str]]
    opportunity_hunter: Dict[str, List[str]]
    research: Dict[str, List[str]]
    startup: Dict[str, List[str]]
    trends: Dict[str, List[str]]
    exclude_keywords: List[str]
    priority_keywords: Dict[str, List[str]]
    platforms: Dict[str, Dict]
    timing: Dict[str, Any]
    output: Dict[str, Any]


class ConfigLoader:
    """é…ç½®æ–‡ä»¶åŠ è½½å™¨"""
    
    def __init__(self, config_dir: str = './config'):
        """
        åˆå§‹åŒ–é…ç½®åŠ è½½å™¨
        
        Args:
            config_dir: é…ç½®æ–‡ä»¶ç›®å½•
        """
        self.config_dir = Path(config_dir)
        self.config_dir.mkdir(parents=True, exist_ok=True)
        self.keywords_file = self.config_dir / 'keywords.yaml'
        self.config_cache = {}
    
    def load_keywords(self, reload: bool = False) -> KeywordsConfig:
        """
        åŠ è½½å…³é”®è¯é…ç½®
        
        Args:
            reload: æ˜¯å¦å¼ºåˆ¶é‡æ–°åŠ è½½
            
        Returns:
            å…³é”®è¯é…ç½®å¯¹è±¡
        """
        if not reload and 'keywords' in self.config_cache:
            logger.debug("ä½¿ç”¨ç¼“å­˜çš„å…³é”®è¯é…ç½®")
            return self.config_cache['keywords']
        
        if not self.keywords_file.exists():
            logger.error(f"å…³é”®è¯é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.keywords_file}")
            return self._get_default_config()
        
        try:
            with open(self.keywords_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            config = KeywordsConfig(
                pain_radar=data.get('pain_radar', {}),
                opportunity_hunter=data.get('opportunity_hunter', {}),
                research=data.get('research', {}),
                startup=data.get('startup', {}),
                trends=data.get('trends', {}),
                exclude_keywords=data.get('exclude_keywords', []),
                priority_keywords=data.get('priority_keywords', {}),
                platforms=data.get('platforms', {}),
                timing=data.get('timing', {}),
                output=data.get('output', {})
            )
            
            self.config_cache['keywords'] = config
            logger.info(f"âœ… æˆåŠŸåŠ è½½å…³é”®è¯é…ç½®: {self.keywords_file}")
            return config
            
        except Exception as e:
            logger.error(f"âŒ åŠ è½½å…³é”®è¯é…ç½®å¤±è´¥: {str(e)}")
            return self._get_default_config()
    
    def _get_default_config(self) -> KeywordsConfig:
        """è·å–é»˜è®¤é…ç½®"""
        return KeywordsConfig(
            pain_radar={
                'chatgpt': ['can\'t', 'doesn\'t work', 'error', 'slow', 'expensive'],
                'claude': ['can\'t', 'doesn\'t support', 'bug', 'rate limit'],
                'deepseek': ['slow', 'error', 'hallucination'],
                'cursor': ['bug', 'crash', 'slow'],
                'midjourney': ['hands weird', 'broken', 'ugly'],
                'sora': ['physics fail', 'movement unnatural', 'face melting'],
            },
            opportunity_hunter={
                'github': ['AI agent', 'RAG', 'prompt engineering', 'automation'],
                'hackernews': ['funding', 'series', 'startup', 'breakthrough'],
                'producthunt': ['AI', 'automation', 'productivity'],
                'reddit': ['looking for', 'need', 'help with', 'bug'],
            },
            research={
                'huggingface': ['Agent', 'Browser Use', 'Optimization', 'Model'],
            },
            startup={
                'ycombinator': ['AI', 'machine learning', 'automation', 'productivity'],
            },
            trends={
                'keywords': ['AI', 'ChatGPT', 'machine learning', 'automation'],
            },
            exclude_keywords=['spam', 'scam', 'fake', 'clickbait'],
            priority_keywords={
                'high': ['funding', 'breakthrough', 'launch'],
                'medium': ['bug', 'error', 'feature'],
                'low': ['question', 'discussion'],
            },
            platforms={
                'github': {'min_stars': 300, 'min_forks': 10},
                'reddit': {'min_score': 10, 'min_comments': 5},
                'twitter': {'min_retweets': 5, 'min_likes': 10},
            },
            timing={'check_interval': 3600, 'retention_days': 90},
            output={'max_results': 100, 'sort_by_priority': True}
        )
    
    def get_pain_radar_keywords(self, product: Optional[str] = None) -> List[str]:
        """
        è·å–ç—›ç‚¹é›·è¾¾å…³é”®è¯
        
        Args:
            product: äº§å“åç§° (å¦‚ 'chatgpt', 'claude')ï¼ŒNone åˆ™è¿”å›æ‰€æœ‰
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        config = self.load_keywords()
        
        if product:
            return config.pain_radar.get(product, [])
        else:
            # è¿”å›æ‰€æœ‰å…³é”®è¯
            all_keywords = []
            for keywords in config.pain_radar.values():
                all_keywords.extend(keywords)
            return all_keywords
    
    def get_opportunity_keywords(self, platform: Optional[str] = None) -> List[str]:
        """
        è·å–æœºä¼šçŒæ‰‹å…³é”®è¯
        
        Args:
            platform: å¹³å°åç§° (å¦‚ 'github', 'hackernews')ï¼ŒNone åˆ™è¿”å›æ‰€æœ‰
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        config = self.load_keywords()
        
        if platform:
            return config.opportunity_hunter.get(platform, [])
        else:
            # è¿”å›æ‰€æœ‰å…³é”®è¯
            all_keywords = []
            for keywords in config.opportunity_hunter.values():
                all_keywords.extend(keywords)
            return all_keywords
    
    def get_exclude_keywords(self) -> List[str]:
        """è·å–æ’é™¤å…³é”®è¯"""
        config = self.load_keywords()
        return config.exclude_keywords
    
    def get_priority_keywords(self, priority: str = 'high') -> List[str]:
        """
        è·å–ä¼˜å…ˆçº§å…³é”®è¯
        
        Args:
            priority: ä¼˜å…ˆçº§ ('high', 'medium', 'low')
            
        Returns:
            å…³é”®è¯åˆ—è¡¨
        """
        config = self.load_keywords()
        return config.priority_keywords.get(priority, [])
    
    def get_platform_config(self, platform: str) -> Dict:
        """
        è·å–å¹³å°ç‰¹å®šé…ç½®
        
        Args:
            platform: å¹³å°åç§°
            
        Returns:
            å¹³å°é…ç½®å­—å…¸
        """
        config = self.load_keywords()
        return config.platforms.get(platform, {})
    
    def add_keyword(self, category: str, subcategory: str, keyword: str) -> bool:
        """
        æ·»åŠ æ–°å…³é”®è¯
        
        Args:
            category: åˆ†ç±» (pain_radar, opportunity_hunter ç­‰)
            subcategory: å­åˆ†ç±» (chatgpt, github ç­‰)
            keyword: å…³é”®è¯
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with open(self.keywords_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            if category not in data:
                data[category] = {}
            
            if subcategory not in data[category]:
                data[category][subcategory] = []
            
            if keyword not in data[category][subcategory]:
                data[category][subcategory].append(keyword)
            
            with open(self.keywords_file, 'w', encoding='utf-8') as f:
                yaml.dump(data, f, allow_unicode=True, default_flow_style=False)
            
            # æ¸…é™¤ç¼“å­˜
            self.config_cache.clear()
            
            logger.info(f"âœ… æ·»åŠ å…³é”®è¯: {category}/{subcategory}/{keyword}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ æ·»åŠ å…³é”®è¯å¤±è´¥: {str(e)}")
            return False
    
    def remove_keyword(self, category: str, subcategory: str, keyword: str) -> bool:
        """
        åˆ é™¤å…³é”®è¯
        
        Args:
            category: åˆ†ç±»
            subcategory: å­åˆ†ç±»
            keyword: å…³é”®è¯
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            with open(self.keywords_file, 'r', encoding='utf-8') as f:
                data = yaml.safe_load(f)
            
            if (category in data and 
                subcategory in data[category] and 
                keyword in data[category][subcategory]):
                
                data[category][subcategory].remove(keyword)
            
            with open(self.keywords_file, 'w', encoding='utf-8') as f:
                yaml.dump(data, f, allow_unicode=True, default_flow_style=False)
            
            # æ¸…é™¤ç¼“å­˜
            self.config_cache.clear()
            
            logger.info(f"âœ… åˆ é™¤å…³é”®è¯: {category}/{subcategory}/{keyword}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ åˆ é™¤å…³é”®è¯å¤±è´¥: {str(e)}")
            return False
    
    def export_to_json(self, output_file: str) -> bool:
        """
        å¯¼å‡ºé…ç½®ä¸º JSON
        
        Args:
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        try:
            config = self.load_keywords()
            
            data = {
                'pain_radar': config.pain_radar,
                'opportunity_hunter': config.opportunity_hunter,
                'research': config.research,
                'startup': config.startup,
                'trends': config.trends,
                'exclude_keywords': config.exclude_keywords,
                'priority_keywords': config.priority_keywords,
                'platforms': config.platforms,
                'timing': config.timing,
                'output': config.output,
            }
            
            with open(output_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… å¯¼å‡ºé…ç½®åˆ° JSON: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¯¼å‡ºé…ç½®å¤±è´¥: {str(e)}")
            return False
    
    def print_summary(self):
        """æ‰“å°é…ç½®æ‘˜è¦"""
        config = self.load_keywords()
        
        print("\n" + "="*60)
        print("ğŸ“‹ å…³é”®è¯é…ç½®æ‘˜è¦")
        print("="*60)
        
        print("\nğŸ”´ ç—›ç‚¹é›·è¾¾å…³é”®è¯:")
        for product, keywords in config.pain_radar.items():
            print(f"  {product}: {len(keywords)} ä¸ªå…³é”®è¯")
        
        print("\nğŸŸ¢ æœºä¼šçŒæ‰‹å…³é”®è¯:")
        for platform, keywords in config.opportunity_hunter.items():
            print(f"  {platform}: {len(keywords)} ä¸ªå…³é”®è¯")
        
        print("\nğŸ”µ ç ”ç©¶å…³é”®è¯:")
        for source, keywords in config.research.items():
            print(f"  {source}: {len(keywords)} ä¸ªå…³é”®è¯")
        
        print("\nğŸŸ¡ åˆ›ä¸šå…³é”®è¯:")
        for source, keywords in config.startup.items():
            print(f"  {source}: {len(keywords)} ä¸ªå…³é”®è¯")
        
        print("\nâšª æ’é™¤å…³é”®è¯:")
        print(f"  å…± {len(config.exclude_keywords)} ä¸ª")
        
        print("\n" + "="*60 + "\n")


def main():
    """æµ‹è¯•é…ç½®åŠ è½½å™¨"""
    logging.basicConfig(level=logging.INFO)
    
    loader = ConfigLoader()
    
    # åŠ è½½é…ç½®
    config = loader.load_keywords()
    
    # æ‰“å°æ‘˜è¦
    loader.print_summary()
    
    # è·å–ç‰¹å®šå…³é”®è¯
    print("ChatGPT ç—›ç‚¹å…³é”®è¯:")
    print(loader.get_pain_radar_keywords('chatgpt'))
    
    print("\nGitHub æœºä¼šå…³é”®è¯:")
    print(loader.get_opportunity_keywords('github'))
    
    # æ·»åŠ æ–°å…³é”®è¯
    print("\n\næ·»åŠ æ–°å…³é”®è¯...")
    loader.add_keyword('pain_radar', 'chatgpt', 'context limit')
    
    # å¯¼å‡ºä¸º JSON
    print("\nå¯¼å‡ºé…ç½®...")
    loader.export_to_json('/tmp/keywords.json')


if __name__ == '__main__':
    main()
